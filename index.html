<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Chenshu Liu</title>
    <meta name="author" content="Chenshu Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <style>
      img {
          width: 100%; /* Make the image responsive to the container width */
          height: auto; /* Maintain the aspect ratio */
      }
    </style>


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-YM02RY2PZY"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-YM02RY2PZY');
    </script>
    
  </head>
 
  <body>

    <!-- <div id="toc">
      <div>diofjaeoif</div>
      <div>diofjaeoif</div>
      <div>diofjaeoif</div>
      <div>diofjaeoif</div>
    </div> -->
    
    <div id="big-container">
      <div id="toc">
        <div><p class="toc_name_padding"><a class = "toc_name" href="#introduction">Introduction</a></p></div>
        <div><p class="toc_name_padding"><a class = "toc_name" href="#research_interest">Research Interest</a></p></div>
        <div><p class="toc_name_padding"><a class = "toc_name" href="#research_projects">Research Projects</a></p></div>
        <div><p class="toc_name_padding"><a class = "indent_toc_name" href="#ongoing_projects">Ongoing Projects</a></p></div>
        <div><p class="toc_name_padding"><a class = "indent_toc_name" href="#2024">2024</a></p></div>
        <div><p class="toc_name_padding"><a class = "indent_toc_name" href="#2023">2023</a></p></div>
        <div><p class="toc_name_padding"><a class = "toc_name" href="#presentation">Talks & Presentations</a></p></div>
        <div><p class="toc_name_padding"><a class = "toc_name" href="#teaching">Teaching</a></p></div>
      </div>

      <table id="big-table" style="margin-left: 20px; margin-right: 20px; width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;"><tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr style="padding:0px">

                <td style="padding:1%;width:35%;max-width:35%">
                  <img style="text-align:center;width:100%;max-width:80%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Personal Photo.jpeg">
                
                  <p style="text-align:center;max-width:80%">
                    &nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/citations?hl=en&user=mZhhswoAAAAJ" target="_blank">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/chenshu-liu-381459175/" target="_blank">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/ChenshuLiu" target="_blank">Github</a>&nbsp;/&nbsp;
                    <a href="./Chenshu_Liu_CV.pdf" download="Chenshu_Liu_CV.pdf">CV</a>
                  </p>

                <td style="padding:20px;width:70%;vertical-align:middle">
                  <p id = "introduction" class="name" style="text-align: center;">
                      Chenshu Liu  (Chen)
                  </p>

                  <p>Currently, I am a Researcher at <a href="https://terasaki.org/institute/">Terasaki Institute</a>, working on wearable biomedical devices and interfaces to integrate AI components in medicine to foster a smart-healthcare ecosystem, supervised by Professor <a href="https://terasaki.org/institute/yangzhi">Yangzhi Zhu</a>, as well as on AI in immunology, supervised by Professor <a href="https://terasaki.org/institute/Chongming-Jiang">Chongming Jiang</a>.</p>
                  <p>I hold a joint appointment at <a href="https://www.ecs.csun.edu/~bingbing/people.html">Laboratory for Smart and Additive Manufacturing</a> at <a href="https://www.csun.edu/">CSUN</a>, working on AI applications in additive manufacturing (AM) and deploying knowledge-graph driven methods for efficient human machine interaction in AM process, supervised by Professor <a href="https://www.ecs.csun.edu/~bingbing/">Bingbing Li</a>.</p> 
                  <p>I also serve as a technical consultant for the <strong>Smart Textile</strong> group led by Professor <a href="https://arcs.center/a-framework-for-smart-textile-large-scale-consumer-research/">Wei Cao</a> at <a href="https://arcs.center/a-framework-for-smart-textile-large-scale-consumer-research/">ARCS</a> on evaluating robustness of wearable smart textiles from the lens of consumer aspects.</p>
                  <p>I did my Masters in <strong>Bioengineering</strong> and a dual bachelor‚Äôs in <strong>Neuroscience</strong> and <strong>Statistics</strong> at University of California, Los Angeles.</p>
                </td> 
              </tr>
            </tbody></table>
            

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
            <div style="background-color: #f6f6f6; padding:.5em 1.5em .5em;">
              <p>I am a neuroscientist üß† and statistician üìä in mind, an engineer ‚öôÔ∏è at hand, and an artist/photographer üì∑ at heart. I am a curious innovator exploring AI integration in Health Science & Humanities, merging computer science, medical engineering, heritage science, neuroscience, etc.
              </p>
            </div>
            <br>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2 id = 'research_interest'>Research Interests</h2>
              <p>
                My research interests focus on creating a <strong>synergy between human and AI</strong>, applying interdisciplinary technology to enhance well-being both physically and spiritually. 
                I come up with engineering innovations to increase the <strong>accessibility</strong> and <strong>intuitiveness</strong> of human-AI interaction in both <strong>healthcare applications</strong> and <strong>societal enrichments</strong>.
              </p>
              <!-- this is a new page -->
              <!-- <p>this is a <a href="test_folder/new_page.html">new page</a></p> -->
              <p>
                I propose to incorporate <strong>knowlege-enhanced multidomain AI</strong> as <strong>decision support</strong> agents. I am interested in constructing reliable knowledge bases (KB) and knowledge graphs (KG) for decision supports with high <strong>interpretability and explainability</strong> in complex decision processes. 
                I am interested in leveraging advanced <strong>sensing</strong> technologies to create a <strong>comprehensive profile</strong> of the subject of interest. By integrating these descriptive inputs and combining them with <strong>domain-specific knowledge</strong>, I aim to enable <strong>more informed and context-aware reasoning</strong>, whether for enhance patient well-being or restoring an artifact. 
                Through the <strong>unification</strong> of sensor data and expert knowledge, I seek to enhance the overall decision-making process.
                I envision working with people from different fields of expertise that can potentially benefitted from AI-aided decision supports and integrate with reliable and efficient pipelines in their workflow.
              </p>
              <p>
                I‚Äôm also interested in rethinking the role of AI in emotion support via <strong>affective computing</strong> and <strong>multi-sensor fusion</strong>, 
                to enable <strong>creativity</strong> and <strong>self-reflection</strong> through the most <strong> natural form of personal expression</strong> and the most <strong>realistic day-to-day behaviors</strong>, which gave rise to the on-going <strong>Posture2Melody</strong> project.
                In the project, I propose to use generative AI, alongside with different modalities of physiology and psychology tracking, as the mirror onto our own understanding of music, lowering the bar for expression.
                This framework invites us to consider how <strong>AI can perceive and make-sense of human emotion overtime</strong>, enabling AI agents to deliver responses appropriately aligned with the user's emotional state. </strong>.
              </p>
              </td>
          </tr>

          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2 id = "research_projects">Research Publications / Projects / Talks</h2>
            <p>
              I have cultivated a broad range domain experiences, spanning both <strong>natural sciences</strong> and <strong>humanities</strong>.
              I am proficient in developing <strong>AI algorithms</strong> diverse application scenarios. The wordcloud below shows the span of my projects, fields of expertise, and techniques.
            </p>
            <img src='images/wordcloud_3.png'>
            <p>Projects are arranged in <strong>chronological</strong> order below, including published works and on-going ones.</p>

            <!-- <p>
              In the healthcare domain, I have worked on projects like developing a smart contact lens, <strong>OPTMISE</strong>, powered by computer vision and time-series analysis algorithms, to support the dynamic monitoring of eye physiology. 
              I also worked on developing AI algorithm for deciphering <strong>single-channel EMG</strong> signals for gesture recognition to support more accessible human-machine interaction, 
              which evolved into the project of combining EMG-decoding and <strong>transcutaneous electrical nerve stimulation (TENS)</strong> to <strong>restore dexterous limb movements in neurodegenerative cases</strong>. 
            </p>
            <p>
              In the realm of social sciences, I have applied my AI knowledge to assess educational tools in higher education settings through <strong>semantic analysis</strong>, providing insights into designing <strong>more effective online pedagogy schemes during the pandemic</strong>. 
              My work in cultural heritage conservation <strong>merges advanced computer vision algorithm</strong> for image processing with <strong>paper-based cultural artifact conservation</strong> to expedite <strong>microorganism-contamination identification</strong> in cultural heritage biodegradation. 
              I also formulated <strong>biochemical cleaning agents</strong> based on surface contaminants and bioenzymes for <strong>removing mold contaminations</strong> from fragile paper-based cultural heritage artifacts, as well as <strong>preserving and enhancing the structural integrity</strong> of the paper medium.
            </p>
            <p>I am also getting involved in developing knowledge-driven reasoning, including the construction of <strong>knowledge graphs (KG)</strong> and <strong>retrival augmented generation (RAG)</strong>, to enhance the intuitiveness of smart manufacturing processes</p> -->
          </td>
            </tbody></table>

  <h2 id="ongoing_projects">On-going Projects</h2>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <colgroup>
      <col class="colone">
      <col class="coltwo">
    </colgroup>
  <tbody>

    <tr>
      <td class="colleft">
        <div class="one">
          <a href="images/Posture2Melody Concept Image.png" target="_blank" style="cursor:pointer"><img src='images/Posture2Melody Concept Image.png'></a>
          <div class="bubble-container"></div>
            <span class="bubble highlight">Human Motion Tracking</span>
            <span class="bubble highlight">Generative AI</span>
            <span class="bubble">Transformer</span>
            <span class="bubble">GAN</span>
            <span class="bubble">Emotion Therapy</span>
          </div>
        </div>
      </td>
      <td class="colright">
        
        <a href="https://github.com/ChenshuLiu/Posture2Melody">
          <span class="papertitle">Posture2Melody: Transforming Movement into Musical Melody with AI Harmony</span>
        </a><br>
        <strong>Chenshu Liu</strong>, Pinyi Yang, Yiran Wang, Haolin Fan.
        <br>
        <br><strong>TLDR:</strong> Posture2Melody uses GAN-Transformer-based architecture to <U>generate melodies from human postures</U>. Inspired by the idea that the expansiveness of human posture reflects emotional states, this project seeks to create a seamless interaction between bodily movement and music. 
        Whether it‚Äôs through dance or everyday postures, <strong>Posture2Melody transforms these movements into musical melodies</strong>, potentially acting as an <u>emotional therapy tool</u>. By synchronizing bodily movement and music, Posture2Melody seeks to develop a creative technique that could be used in emotional therapy.<br>
      </td>
    </tr> 

    <tr>
      <td class="colleft">
        <div class="one">
          <a href="images/NeuroMT.png" target="_blank" style="cursor:pointer"></a><img src='images/NeuroMT.png'></a>
          <div class="bubble-container"></div>
            <span class="bubble highlight">Electromyography</span>
            <span class="bubble highlight">Transcutaneous Electrical Stimulation</span>
            <span class="bubble">Gesture Recognition</span>
            <span class="bubble">Neurodegenerative Disease</span>
            <span class="bubble">Theranostics System</span>
          </div>
        </div>
      </td>
      <td class="colright">
        
        <a href="">
          <span class="papertitle">NeuroMT: Neurodegenerative Monitoring and Therapy Device Using EMG and TENS</span>
        </a><br>
        <strong>Chenshu Liu</strong>, Pinyi Yang, Yiran Wang, Haolin Fan, Ziyuan Che, Yangzhi Zhu, Bingbing Li.
        <br>
        <br><strong>TLDR:</strong> The NeuroMT project introduces an innovative <strong>theranostic device</strong> combining <strong>electromyography (EMG)</strong> and <strong>Transcutaneous Electrical Nerve Stimulation (TENS) unit</strong> to monitor and treat abnormal neuromuscular activity in real-time. 
        By leveraging single-channel EMG, the <u>system detects irregularities in muscle activation associated with neurodegenerative disorders</u>. Subsequently, it <u>employs TENS unit to modulate neuromuscular pathways and restore stable motor function</u>.
        This closed-loop system integrates both diagnostics and therapy, enabling real-time feedback and personalized treatment. 
        Our approach offers a novel solution for <strong>neurodegenerative patients</strong>, potentially improving mobility, reducing symptoms, and enhancing overall quality of life.<br>
      </td>
    </tr> 

    <tr>
      <td class="colleft">
        <div class="one">
          <a href="images/metalmind.png" target = "_blank" style="cursor:pointer"><img src='images/metalmind.png'></a>
          <div class="bubble-container"></div>
            <span class="bubble highlight">Electromyography</span>
            <span class="bubble highlight">Transcutaneous Electrical Stimulation</span>
            <span class="bubble">Gesture Recognition</span>
            <span class="bubble">Neurodegenerative Disease</span>
            <span class="bubble">Theranostics System</span>
          </div>
        </div>
      </td>
      <td class="colright">
        
        <a href="">
          <span class="papertitle">MetalMind: A Knowledge Graph-Based Retrieval Framework for Enhanced Human-Machine Interaction in Metal Additive Manufacturing</span>
        </a><br>
        Haolin Fan, Xinyu Liu, Zhen Fan, <strong>Chenshu Liu</strong>, Jerry Ying Hsi Fuh, Wen Feng Lu, Bingbing Li. <em>53rd SME North American Manufacturing Research Conference (In Preparation), 2025</em>
        <br>
        <br><strong>TLDR:</strong> This study presents a multi-modal Knowledge Graph (KG) combined with Retrieval-Augmented Generation (RAG) to enhance HMI in Digital Twin (DT) systems, facilitating structured, semantically enriched access to metal AM knowledge. 
        Our system supports training and decision-making through an automated KG construction pipeline encompassing preprocessing, data extraction using Large Language Models (LLMs), and post-processing with collaborative verification to ensure quality. 
        We implemented three retrieval modes: vector-based for detailed queries, graph-based for contextual insights, and a hybrid method for balanced information retrieval. 
        Additionally, a KG-based image retrieval feature connects entity descriptions to relevant visual data. 
        Experimental results indicate that the KG-driven hybrid retrieval mode enhances both global and granular understanding. 
        This publicly accessible system establishes a foundation for advanced HMI in smart manufacturing.<br>
      </td>
    </tr> 
    
  </tbody> 
  </table>

  <h2 id="2024">2024</h2>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <colgroup>
      <col class="colone">
      <col class="coltwo">
    </colgroup>
    <tbody>

  <tr> 
    <td class="colleft">
      <div class="one">
        <a href = "images/OPTMISE_full_image.png" target = "_blank" class = "image_show"><img src='images/OPTMISE Lens Schematic.png'></a>
        <div class="bubble-container">
          <span class="bubble highlight">Wearable</span>
          <span class="bubble highlight">Computer Vision</span>
          <span class="bubble">RNN</span>
          <span class="bubble">Smart Contact Lens</span>
          <span class="bubble">Color Calibration</span>
          <span class="bubble">Diagnostic Platform</span>
        </div>
      </div>
    </td>
    <td class="colright">
      <a href="https://github.com/ChenshuLiu/OPTMISE">
        <span class="papertitle">OPTMISE: Ocular Platform with Telemetric Mechano-Electro-Chromic Intelligent Sensing Ecosystem</span>
      </a><br>
      <strong>Chenshu Liu</strong>, Hyo-Jeong Choi, Chengguang Zhang, Pengrui Dang, Wangjie Chen, Yongju Lee, Bingbing Li, Meyer Dawn, Pete Kollbaum, Hyeok Kim, Ali Khademhosseini, Yangzhi Zhu. <em>Nature Biomedical Engineering (under review), 2024</em><br>
      <br><strong>TLDR:</strong> The OPTMISE lens offers a <strong>self-powered minimal-invasive wearable</strong> measuring alternative for <u>measuring eyelid pressure</u> based on Triboelectric Nanogenerator (TENG). 
      In constrast to traditional eyelid measurement methods that involve setting non-conformable foreign measuring apparatus between the eyeball and the eyelid, <strong>OPTMISE lens is a stand alone apparatus, making the device self-contained and less invasive</strong>. 
      User can directly wear the OPTMISE lens in the <strong>same way as a regular contact lens</strong>. The colorimetric mechano-chomic display (composed of PEDOT:PSS material) provides visual qualification of different eyelid pressures.
      A customized software based on computer vision and time series processing AI algorithm creates an interface to allow users to dynamically quantify the eye pressure captured by the mechano-chomic display.<br>
    </td>
  </tr>     

  <tr> 
    <td class="colleft">
      <div class="one">
        <a href="images/Multidomain AI + Biosensor.png" target="_blank" class = "image_show"><img src='images/Multidomain AI + Biosensor.png'></a>
        <div class="bubble-container"></div>
          <span class="bubble highlight">Electronic Skin</span>
          <span class="bubble highlight">Multidomain AI</span>
          <span class="bubble">Decentralized Healthcare</span>
          <span class="bubble">Electronic Health Record</span>
          <span class="bubble">Decision Support System</span>
        </div>
      </div>
    </td>
    <td class="colright">
      <a href="images/multimodal_ai_wearable.png">
        <span class="papertitle">Democratizing Healthcare: The Synergy of Electronic Skin and Multidomain AI</span>
      </a><br>
      <strong>Chenshu Liu</strong>, Pinyi Yang, Tong Zhou, Haolin Fan, Lingdi Zhao, Yiran Wang, Yangzhi Zhu, Bingbing Li. <em>ACS Chemical Reviews (submitted), 2024</em><br>
      <br><strong>TLDR:</strong> Electronic skin (E-skin) is revolutionizing the monitoring of physiological states. 
      With artificial intelligence (AI), sensory data from E-skin can now be decoded to provide valuable health insights. 
      However, current E-skin and AI integrations are largely unimodal, which limits the ability comprehensively represent subject's overall physiological state. 
      Furthermore, most AI algorithms did not take advantage of the large body of empirical data and domain knowledge to offer deeper clinical decision support. 
      Thus, this review proposes a framework to accelerate the shift toward <strong>decentralized healthcare</strong> by <strong>fostering synergy between multidomain AI and multimodal E-skin</strong>. 
      We highlight various E-skin modalities and using multi-sensor fusion AI technologies to create comprehensive <u>patient profiles</u>. 
      Additionally, we discuss how multimodal AI can improve medical reasoning and patient outcomes by integrating EHR data and domain-specific knowledge. 
      We explore the future potential and challenges of the multidomain AI and E-skin partnership, emphasizing the personalized healthcare benefits this collaboration can deliver.<br>
    </td>
  </tr>    

  <tr>
    <td class="colleft">
      <div class="one">
        <a href="images/practical_experiment_1.jpg" target="_blank" class="image_show"><img src='images/practical_experiment_1.jpg'></a>
        <div class="bubble-container"></div>
          <span class="bubble highlight">Smart Manufacturing</span>
          <span class="bubble highlight">Vision Language Model</span>
          <span class="bubble">Decision Support System</span>
        </div>
      </div>
    </td>
    <td class="colright">
      <a href="">
        <span class="papertitle">MaViLa: Unlocking New Potentials in Smart Manufacturing through Vision Language Models</span>
      </a><br>
      Haolin Fan, <strong>Chenshu Liu</strong>, Neville Elieh Janvisloo ,Shijie Bian ,Jerry Ying Hsi Fuh ,Wen Feng Lu ,Bingbing Li. <em>Journal of Manufacturing Systems  (under review), 2024</em><br>
      <br><strong>TLDR:</strong> This paper presents MaViLa, a novel Vision Language Model (VLM) designed to unlock new potentials in smart manufacturing. 
      Through rigorous methodology and extensive experiments, MaViLa demonstrates superior performance across various benchmarks compared to other general-purpose VLMs. 
      This enhanced grasping of domain knowledge is attributed to the innovative use of an external vector store during the dataset construction process. 
      Practical experiments, including lab tests and the application of the CAXTON dataset, reveal that MaViLa excels in manufacturing tasks.<br>
    </td>
  </tr>

  <tr>
    <td class="colleft">
      <div class="one">
        <a href="images/AI_in_AM.png" target="_blank" class="image_show"><img src='images/AI in AM.png'></a>
        <div class="bubble-container"></div>
          <span class="bubble highlight">Smart Manufacturing</span>
          <span class="bubble highlight">Multimodal AI</span>
          <span class="bubble">Knowledge Graph</span>
          <span class="bubble">Decision Support System</span>
        </div>
      </div>
    </td>
    <td class="colright">
      <a href="images/AI_in_AM.png">
        <span class="papertitle">New Era Towards Autonomous Additive Manufacturing: A Review of Recent Trends and Future Perspectives</span>
      </a><br>
      Haolin Fan, <strong>Chenshu Liu</strong>, Shijie Bian, Changyu Ma, Xuan Liu, Marshall Doyle, Thomas Lu, Lianyi Chen, Jerry Ying Hsi Fuh, Wen Feng Lu, Bingbing Li. <em>International Journal of Extreme Manufacturing (accepted for publication), 2024</em><br>
      <br><strong>TLDR:</strong> The Additive Manufacturing (AM) landscape has significantly transformed in alignment with Industry 4.0 principles, primarily driven by the integration of Artificial Intelligence (AI) and Digital Twin (DT). 
      This review paper examines current solutions in Intelligent Additive Manufacturing, emphasizing control, monitoring and process autonomy, and end-to-end process integration. 
      This paper addresses the lifelong learning and self-optimization capabilities of AI agents. As manufacturing evolves, this paper posits that the future of AM will be characterized by a symbiotic relationship between advanced autonomy and human expertise, fostering a more adaptive and autonomous future manufacturing ecosystem.<br>
    </td>
  </tr> 

  <tr>
    <td class="colleft">
      <div class="one">
        <a href="images/AI + Microorganism Identification.png" target="_blank" class="image_show"><img src='images/AI + Microorganism Identification.png'></a>
        <div class="bubble-container"></div>
          <span class="bubble highlight">Computer-aided Diagnostics</span>
          <span class="bubble highlight">Cultural Heritage Conservation</span>
          <span class="bubble">Computer Vision</span>
          <span class="bubble">Knowledge Base</span>
        </div>
      </div>
    </td>
    <td class="colright">
      <a href="https://link.springer.com/article/10.1186/s40494-024-01267-5">
        <span class="papertitle">Web-based diagnostic platform for microorganism-induced deterioration on paper-based cultural relics with iterative training from human feedback</span>
      </a><br>
      <strong>Chenshu Liu*</strong>, Songbin Ben, Chongwen Liu, Xianchao Li, Qingxia Meng, Yilin Hao, Qian Jiao, Pinyi Yang. <em>Heritage Science, 2024</em><br>
      <br><strong>TLDR:</strong> Paper-based artifacts hold significant cultural and social values. 
      However, paper is intrinsically fragile to microorganisms, such as mold, due to its cellulose composition. Mold not only can damage papers‚Äô structural integrity and pose significant challenges to conservation works
      but also may subject individuals attending the contaminated artifacts to health risks. Current conservation practices with mold-contaminated artifacts have
      little to no pre-screening, and the cleaning techniques are usually broad-spectrum rather than strain-specific. 
      This study investigated the feasibility of using a convolutional neural network (CNN) for <strong>fast in-situ recognition and classification of mold species</strong> on paper.
      A <a href="https://biodegrade-diagnostics.streamlit.app/">webtool</a> deployed via <a href="https://streamlit.io/">Streamlit</a> was developed to provide public access to the mold classification tool, alongside with decision support system that provide detailed description about the strain of mold detected.<br>
    </td>
  </tr> 

  <tr>
    <td class="colleft">
      <div class="one">
        <img src='images/Bird Song Classification.png'>
        <div class="bubble-container"></div>
          <span class="bubble highlight">Classification Model</span>
          <span class="bubble highlight">Audio Processing</span>
          <span class="bubble">Machine Learning</span>
        </div>
      </div>
    </td>
    <td class="colright">
      <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0297988#sec002">
        <span class="papertitle">Recognition of bird species with birdsong records using machine learning methods</span>
      </a><br>
      Yi Tang, <strong>Chenshu Liu</strong>, Xiang Yuan. <em>Plos one, 2024</em><br>
      <br><strong>TLDR:</strong> The recognition of bird species through the analysis of their vocalizations is a crucial aspect of wildlife conservation and biodiversity monitoring. In this study, the acoustic features of Certhia americana, Certhia brachydactyla, and Certhia familiaris were calculated to train three machine learning models, Random Forest (RF), Support Vector Machine (SVM), and Extreme Gradient Boosting (XGBoost). The XGBoost model had the best performance among the three models, with the highest accuracy and the highest AUC. The study provides a new approach to bird species recognition that utilizes sound data and acoustic characteristics.<br>
    </td>
  </tr>  

  <tr>
    <td class="colleft">
      <div class="one">
        <img src='images/Education.png'>
        <div class="bubble-container"></div>
          <span class="bubble highlight">Natural Langauge Processing</span>
          <span class="bubble highlight">Semantics Classification</span>
          <span class="bubble">Online Pedagogy</span>
          <span class="bubble">COVID-19 Pandemic</span>
        </div>
      </div>
    </td>
    <td class="colright">
      <a href="https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1326284/full">
        <span class="papertitle">A practical evaluation of online self-assisted previewing architecture on rain classroom for biochemistry lab courses</span>
      </a><br>
      <strong>Chenshu Liu*</strong>, Songbin Ben*, Pinyi Yang, Jiayi Gong, Yin He. <em>Frontiers in Education, 2024</em><br>
      <br><strong>TLDR:</strong> This study investigated the design of the optimal structure of online self-assisting coursework for laboratory courses that can assist students to better prepare for hands-on experiments. 
      Survey was conducted among undergraduate students who took Biochemistry during and post-pandemic. Textmining and semantics classification were performed on students' responses to analyze their emotions towards established online pedagogy frameworks and gain insights in their suggestion for a more effective online learning platform design. 
      We offer a few strategic suggestions that may guide the design of future online resources for laboratory classes such as involving multi-modality media to improve engagement and perfecting the interactive feature to increase its usage by students.<br>
    </td>
  </tr>

  <tr>
    <td class="colleft">
      <div class="one">
        <img src='images/Lucas Museum.jpg'>
        <div class="bubble-container"></div>
          <span class="bubble highlight">Computer-aided Diagnostics</span>
          <span class="bubble highlight">Cultural Heritage Conservation</span>
          <span class="bubble highlight">Paper Artifact Conservation</span>
          <span class="bubble">Computer Vision</span>
        </div>
      </div>
    </td>
    <td class="colright">
      <a href="https://github.com/ChenshuLiu/Biodegrade-Diagnostics">
        <span class="papertitle">Ai-Assisted Classification of Microorganism Strains on Paper-Based Cultural Relics (Workshop Presentation)</span>
      </a><br>
      <strong>Chenshu Liu*</strong>, Chongwen Liu, Allison Wall. <em>Lucas Museum Paper Conservation Workshop</em><br>
      <br><strong>TLDR:</strong> Invited to showcase integration of computer vision algorithms in the practice of conservation at the paper conservation workshop held at the <a href="https://lucasmuseum.org/">Lucas Museum of Narrative Art</a> hosted by <a href="https://losangelesartconservation.com/about/">Erin Jue</a>. 
      Our method focused on using miscroscopic images of mold stains on paper-based cultural relics using computer vision algorithm.<br>
    </td>
  </tr>

    </tbody>
  </table>

  <h2 id="2023">2023</h2>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <colgroup>
      <col class="colone">
      <col class="coltwo">
    </colgroup>
    <tbody>

  <tr> 
    <td class="colleft">
      <div class="one">
        <img src='images/Microorganism Cleaning.png'>
        <div class="bubble-container"></div>
          <span class="bubble highlight">Cultural Heritage Conservation</span>
          <span class="bubble highlight">Mold Removal</span>
          <span class="bubble">Surface Cleaning Agent</span>
          <span class="bubble">Surfactant</span>
          <span class="bubble">Bioenzyme</span>
        </div>
      </div>
    </td>
    <td class="colright">
      <a href="https://link.springer.com/article/10.1186/s40494-023-01083-3">
        <span class="papertitle">A biological cleaning agent for removing mold stains from paper artifacts</span>
      </a><br>
      Qingxia Meng, Xianchao Li, Junqiang Geng, <strong>Chenshu Liu*</strong>, Songbin Ben*. <em>Heritage Science, 2023</em><br>
      <br><strong>TLDR:</strong> Efficient removal of mold stains becomes an important research topic for paper conservation. 
      In this study, a cleaning scheme based on the combination of bioenzymes and biosurfactants was explored. 
      A cleaning agent composed of Sophorolipid and Betaine offer superior deacidification, anti-acidification, anti-aging, and reinforcement capabilities, which can provide extra support to the fibrous structure in addition to cleaning the paper materials. 
      The microbial contamination cleaning agent proposed in this study shows promising application prospects in conserving mold-contaminated paper artifacts.<br>
    </td>
  </tr> 

  <tr> 
    <td class="colleft">
      <div class="one">
        <img src='images/ABM.JPG'>
        <div class="bubble-container"></div>
          <span class="bubble highlight">Cultural Heritage Conservation</span>
          <span class="bubble highlight">Mold Identification</span>
          <span class="bubble">Computer Vision</span>
          <span class="bubble">Convolutional Neural Network</span>
        </div>
      </div>
    </td>
    <td class="colright">
      <a href="https://www.artbiomatters.org/allmembers/chenshu-liu">
        <span class="papertitle">Ai-Assisted Classification of Microorganism Strains on Paper-Based Cultural Relics (Conference Presentation)</span>
      </a><br>
      <strong>Chenshu Liu</strong>, Chongwen Liu, Allison Wall. <em>Art Bio Matters (ABM) Conference, 2023</em><br>
      <br><strong>TLDR:</strong> Our project that focused on using miscroscopic images of mold stains on paper-based cultural relics using computer vision algorithm was selected to present at the <a href="https://www.artbiomatters.org/">Art Bio Matters</a> conference. 
      The project gained recognition in the cultural heritage conservation community and hold promises in assisting diagnostic procedure in paper-based cultural relic biodeterioration conservation.<br>
    </td>
  </tr> 

  </tbody>
  </table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
          
      <div id="recruiting">  
        <h2 id = "presentation" style="padding-top:40px;">Talks and Presentations</h2>
      </div>
      <p>I have been invited and attending conferences, here is the list of the events I have attended and corresponding recorded/pre-recorded videos:</p>
      <ul>
        <li><a href="https://www.youtube.com/watch?v=O0tXfUGOlEM">ABM Conference Pre-recorded Presentation (2023)</a></li>
        <li><a href="https://www.youtube.com/watch?v=V7fwydg6NVQ&t=616s">NSF I-Corps Proposal Video Demo (2023)</a></li>
        <li><a href="./images/Lucas Museum.jpg">Lucas Museum Paper Conservation Workshop Invited Talk (2024)</a></li>
      </ul>
    </td>
  </tr>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
          
      <div id="recruiting">  
        <h2 id = "teaching" style="padding-top:40px;">Teaching</h2>
      </div>

      <p>
      I am devoted to education of all education levels. I have been teaching for <strong>over 9 years</strong> since the start of senior high. I have taught a broad range of courses, covering subjects: biology (AP, college level), chemistry (AP), calculus (AP, college level), statistics (AP, college level), programming in R, programming in Python, etc. I was the TA for <a href="https://ls7l.lscore.ucla.edu/LAB/">LS23L: Laboratory and Scientific Methods </a> at UCLA in the 2022-2023 academic year.<br>
      </p>
      <p>I am also devoted to enhance public understanding of machine learning and artificial intelligence. I have a personal education channel over <a href="https://www.xiaohongshu.com/user/profile/625a1c60000000000c036ad4">RED</a> to share bite-size knowledge of ML algorithms and AI-related techniques. I also curate a suite of ML-education repositories on Github:</p>
      <ul>
        <li><a href="https://github.com/ChenshuLiu/Machine-Learning-in-Python.git">Machine learning from scratch in Python</a></li>
        <li><a href="https://github.com/ChenshuLiu/Machine-Learning-in-R.git">Machine learning application in R</a></li>
        <li><a href="https://github.com/ChenshuLiu/Deep-Learning-with-R.git">Deep learning application in R</a></li>
      </ul>
    </td>
  </tr>

    </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px">
            <br>
            <p style="font-size:small;">
              This website was built thanks to the help of <a href="https://github.com/czhuang/czhuang.github.io">this</a> source code.
            </p>
          </td>
        </tr>
      </tbody></table>
      </td>
      </tr>
    </table>

    <div id="toc">
    </div>

  </div>
  </body>
</html>
