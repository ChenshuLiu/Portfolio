<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Chenshu Liu</title>
    <meta name="author" content="Chenshu Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <style>
      img {
          width: 100%; /* Make the image responsive to the container width */
          height: auto; /* Maintain the aspect ratio */
      }
    </style>


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-YM02RY2PZY"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-YM02RY2PZY');
    </script>
    
  </head>
 
  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">

              <td style="padding:1%;width:35%;max-width:35%">
                <img style="text-align:center;width:100%;max-width:80%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Personal Photo.jpeg">
              
                <p style="text-align:center;max-width:80%">
                  &nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/citations?hl=en&user=mZhhswoAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/chenshu-liu-381459175/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ChenshuLiu">Github</a>
                </p>

              <td style="padding:20px;width:70%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                    Chenshu Liu  (Chen)
                </p>

                <p>Currently, I am a Researcher at <a href="https://terasaki.org/institute/">Terasaki Institute</a>, working on wearable biomedical devices and interfaces to integrate AI components in medicine to foster a smart-healthcare ecosystem, supervised by Professor <a href="https://terasaki.org/institute/yangzhi">Yangzhi Zhu</a>, as well as on AI in immunology, supervised by Professor <a href="https://terasaki.org/institute/Chongming-Jiang">Chongming Jiang</a>.</p>
                <p>I hold a joint appointment at <a href="https://www.ecs.csun.edu/~bingbing/people.html">Laboratory for Smart and Additive Manufacturing</a> at <a href="https://www.csun.edu/">CSUN</a>, working on AI applications in additive manufacturing (AM) and deploying knowledge-graph driven methods for efficient human machine interaction in AM process, supervised by Professor <a href="https://www.ecs.csun.edu/~bingbing/">Bingbing Li</a>.</p> 
                <p>I also serve as a technical consultant for the <strong>Smart Textile</strong> group led by Professor <a href="https://arcs.center/a-framework-for-smart-textile-large-scale-consumer-research/">Wei Cao</a> at <a href="https://arcs.center/a-framework-for-smart-textile-large-scale-consumer-research/">ARCS</a> on evaluating robustness of wearable smart textiles from the lens of consumer aspects.</p>
                <p>I did my Masters in <strong>Bioengineering</strong> and a dual bachelor‚Äôs in <strong>Neuroscience</strong> and <strong>Statistics</strong> at University of California, Los Angeles.</p>
              </td> 
            </tr>
          </tbody></table>
          

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         <tr>
          <td style="padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
          <div style="background-color: #f6f6f6; padding:.5em 1.5em .5em;">
            <p>I am a neuroscientist üß† and statistician üìä in mind, an engineer ‚öôÔ∏è at hand, and an artist/photographer üì∑ at heart. I am a curious innovator exploring AI integration in Health Science & Humanities, merging computer science, medical engineering, heritage science, neuroscience, etc.
            </p>
          </div>
          <br>
          </td>
         </tr>
         <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Research Interests</h2>
            <p>
              My research interests focus on creating a <strong>synergy between human and AI</strong>, applying interdisciplinary technology to enhance well-being both physically and spiritually. 
              I come up with engineering innovations to increase the <strong>accessibility</strong> and <strong>intuitiveness</strong> of human-AI interaction in both <strong>healthcare applications</strong> and <strong>societal enrichments</strong>.
            </p>
            <p>
              I propose to incorporate <strong>knowlege-enhanced multidomain AI</strong> as <strong>decision support</strong> agents. I am interested in constructing reliable knowledge bases (KB) and knowledge graphs (KG) for decision supports with high <strong>interpretability and explainability</strong> in complex decision processes. 
              I am interested in leveraging advanced <strong>sensing</strong> technologies to create a <strong>comprehensive profile</strong> of the subject of interest. By integrating these descriptive inputs and combining them with <strong>domain-specific knowledge</strong>, I aim to enable <strong>more informed and context-aware reasoning</strong>, whether for enhance patient well-being or restoring an artifact. 
              Through the <strong>unification</strong> of sensor data and expert knowledge, I seek to enhance the overall decision-making process.
              I envision working with people from different fields of expertise that can potentially benefitted from AI-aided decision supports and integrate with reliable and efficient pipelines in their workflow.
            </p>
            <p>
              I‚Äôm also interested in rethinking the role of AI in emotion support via <strong>affective computing</strong> and <strong>multi-sensor fusion</strong>, 
              to enable <strong>creativity</strong> and <strong>self-reflection</strong> through the most <strong> natural form of personal expression</strong> and the most <strong>realistic day-to-day behaviors</strong>, which gave rise to the on-going <strong>Posture2Melody</strong> project.
              In the project, I propose to use generative AI, alongside with different modalities of physiology and psychology tracking, as the mirror onto our own understanding of music, lowering the bar for expression.
              This framework invites us to consider how <strong>AI can perceive and make-sense of human emotion overtime</strong>, enabling AI agents to deliver responses appropriately aligned with the user's emotional state. </strong>.
            </p>
            </td>
         </tr>

        <td style="padding:20px;width:100%;vertical-align:middle">
          <h2>Research Projects</h2>
          <p>
            I have cultivated a broad range domain experiences, spanning both <strong>natural sciences</strong> and <strong>humanities</strong>.
            I am proficient in developing <strong>AI algorithms</strong> diverse application scenarios. The wordcloud below shows the span of my projects, fields of expertise, and techniques.
          </p>
          <img src='images/wordcloud_3.png'>
          <p>Projects are arranged in <strong>chronological</strong> order below, including published works and on-going ones.</p>

          <!-- <p>
            In the healthcare domain, I have worked on projects like developing a smart contact lens, <strong>OPTMISE</strong>, powered by computer vision and time-series analysis algorithms, to support the dynamic monitoring of eye physiology. 
            I also worked on developing AI algorithm for deciphering <strong>single-channel EMG</strong> signals for gesture recognition to support more accessible human-machine interaction, 
            which evolved into the project of combining EMG-decoding and <strong>transcutaneous electrical nerve stimulation (TENS)</strong> to <strong>restore dexterous limb movements in neurodegenerative cases</strong>. 
          </p>
          <p>
            In the realm of social sciences, I have applied my AI knowledge to assess educational tools in higher education settings through <strong>semantic analysis</strong>, providing insights into designing <strong>more effective online pedagogy schemes during the pandemic</strong>. 
            My work in cultural heritage conservation <strong>merges advanced computer vision algorithm</strong> for image processing with <strong>paper-based cultural artifact conservation</strong> to expedite <strong>microorganism-contamination identification</strong> in cultural heritage biodegradation. 
            I also formulated <strong>biochemical cleaning agents</strong> based on surface contaminants and bioenzymes for <strong>removing mold contaminations</strong> from fragile paper-based cultural heritage artifacts, as well as <strong>preserving and enhancing the structural integrity</strong> of the paper medium.
          </p>
          <p>I am also getting involved in developing knowledge-driven reasoning, including the construction of <strong>knowledge graphs (KG)</strong> and <strong>retrival augmented generation (RAG)</strong>, to enhance the intuitiveness of smart manufacturing processes</p> -->
        </td>
          </tbody></table>

<h2>On-going Projects</h2>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <colgroup>
    <col class="colone">
    <col class="coltwo">
  </colgroup>
<tbody>

  <tr>
    <td class="colleft">
      <div class="one">
        <img src='images/Posture2Melody Concept Image.png'>
        <div class="bubble-container"></div>
          <span class="bubble highlight">Human Motion Tracking</span>
          <span class="bubble highlight">Generative AI</span>
          <span class="bubble">Transformer</span>
          <span class="bubble">GAN</span>
          <span class="bubble">Emotion Therapy</span>
        </div>
      </div>
    </td>
    <td class="colright">
      
      <a href="https://github.com/ChenshuLiu/Posture2Melody">
        <span class="papertitle">Posture2Melody: Transforming Movement into Musical Melody with AI Harmony</span>
      </a><br>
      <strong>Chenshu Liu</strong>, Pinyi Yang, Yiran Wang, Haolin Fan.
      <br>
      <br><strong>TLDR:</strong> Posture2Melody uses GAN-Transformer-based architecture to generate melodies from human postures. Inspired by the idea that the expansiveness of human posture reflects emotional states, this project seeks to create a seamless interaction between bodily movement and music. 
      Whether it‚Äôs through dance or everyday postures, Posture2Melody transforms these movements into melodies, potentially acting as an emotional therapy tool. By synchronizing bodily movement and music, Posture2Melody seeks to develop a creative technique that could be used in emotional therapy.<br>
    </td>
  </tr> 

  <tr>
    <td class="colleft">
      <div class="one">
        <img src='images/NeuroMT.png'>
        <div class="bubble-container"></div>
          <span class="bubble highlight">Electromyography</span>
          <span class="bubble highlight">Transcutaneous Electrical Stimulation</span>
          <span class="bubble">Gesture Recognition</span>
          <span class="bubble">Neurodegenerative Disease</span>
          <span class="bubble">Theranostics System</span>
        </div>
      </div>
    </td>
    <td class="colright">
      
      <a href="">
        <span class="papertitle">NeuroMT: Neurodegenerative Monitoring and Therapy Device Using EMG and TENS</span>
      </a><br>
      <strong>Chenshu Liu</strong>, Pinyi Yang, Yiran Wang, Haolin Fan, Ziyuan Che, Yangzhi Zhu, Bingbing Li.
      <br>
      <br><strong>TLDR:</strong> The NeuroMT project introduces an innovative theranostic device designed to monitor and treat abnormal neuromuscular activity in real-time. 
      By leveraging single-channel electromyography (EMG), the system detects irregularities in muscle activation associated with neurodegenerative disorders, providing continuous, non-invasive monitoring of limb movements. 
      The device subsequently employs Transcutaneous Electrical Nerve Stimulation (TENS) unit to modulate neural pathways and restore motor function.
      This closed-loop system integrates both diagnostics and therapy, enabling real-time feedback and personalized treatment. 
      By continuously adjusting TENS parameters based on EMG signals, the device optimizes therapeutic interventions, targeting abnormal muscle activation patterns. 
      Our approach offers a novel solution for neurodegenerative patients, potentially improving mobility, reducing symptoms, and enhancing overall quality of life. The paper presents the underlying framework, including EMG signal analysis, the feedback loop for TENS modulation, and preliminary results demonstrating the system‚Äôs effectiveness in restoring normal motor function. GlobalEMG represents a significant advancement in theranostic technologies, promising a new era of integrated monitoring and treatment for neurodegenerative diseases.<br>
    </td>
  </tr> 
  
</tbody> 
</table>

<h2>2024</h2>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <colgroup>
    <col class="colone">
    <col class="coltwo">
  </colgroup>
  <tbody>

<tr> 
  <td class="colleft">
    <div class="one">
      <img src='images/OPTMISE Lens Schematic.png'>
      <div class="bubble-container">
        <span class="bubble highlight">Wearable</span>
        <span class="bubble highlight">Computer Vision</span>
        <span class="bubble">RNN</span>
        <span class="bubble">Color Calibration</span>
      </div>
    </div>
  </td>
  <td class="colright">
    <a href="https://github.com/ChenshuLiu/OPTMISE">
      <span class="papertitle">OPTMISE: Ocular Platform with Telemetric Mechano-Electro-Chromic Intelligent Sensing Ecosystem</span>
    </a><br>
	  <strong>Chenshu Liu</strong>, Hyo-Jeong Choi, Chengguang Zhang, Pengrui Dang, Wangjie Chen, Yongju Lee, Bingbing Li, Meyer Dawn, Pete Kollbaum, Hyeok Kim, Ali Khademhosseini, Yangzhi Zhu. <em>Nature Electronics (under review), 2024</em><br>
    <br><strong>TLDR:</strong> The OPTMISE lens offers an minimal-invasive measuring alternative to traditional eyelid pressure measurement methods that involve setting non-conformable foreign measuring apparatus between the eyeball and the eyelid. 
    A customized software based on computer vision and time series processing AI algorithm creates an interface to allow users to dynamically analyze the eye pressure captured by the lens.<br>
  </td>
</tr>     

<tr> 
  <td class="colleft">
    <div class="one">
      <img src='images/Multidomain AI + Biosensor.png'>
      <div class="bubble-container"></div>
        <span class="bubble highlight">Wearable Biosensor</span>
        <span class="bubble highlight">Multidomain AI</span>
        <span class="bubble">Personalized Healthcare</span>
      </div>
    </div>
  </td>
  <td class="colright">
    <a href="">
      <span class="papertitle">Democratizing Healthcare: The Synergy of Smart Sensing Technologies and Multidomain AI for Proactive Personalized Healthcare: Promises \& Challenges</span>
    </a><br>
    <strong>Chenshu Liu</strong>, Pinyi Yang, Tong Zhou, Haolin Fan, Lingdi Zhao, Yiran Wang, Yangzhi Zhu, Bingbing Li. <em>Chemical Reviews (under review), 2024</em><br>
    <br><strong>TLDR:</strong> User-friendly biosensors and the rise of multidomain artificial intelligent hold the potential to revolutionize patient care by providing more accessible and precise health monitoring solutions, empowering patients to take a proactive role in their health. 
    Advanced biosensors enable continuous and real-time health monitoring, allowing for early detection and management of diseases. 
    Meanwhile, multidomain AI, not only integrates data from various biosensing modalities for comprehensive physiology profiling, but also can transfer knowledge across different domains of expertise, enhancing medical reasoning processes.<br>
  </td>
</tr>  

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/AI + Microorganism Identification.png'>
      <div class="bubble-container"></div>
        <span class="bubble highlight">Computer-aided Diagnostics</span>
        <span class="bubble highlight">Cultural Heritage Conservation</span>
        <span class="bubble">Computer Vision</span>
        <span class="bubble">Knowledge Base</span>
      </div>
    </div>
  </td>
  <td class="colright">
    <a href="https://link.springer.com/article/10.1186/s40494-024-01267-5">
      <span class="papertitle">Web-based diagnostic platform for microorganism-induced deterioration on paper-based cultural relics with iterative training from human feedback</span>
    </a><br>
    <strong>Chenshu Liu*</strong>, Songbin Ben, Chongwen Liu, Xianchao Li, Qingxia Meng, Yilin Hao, Qian Jiao, Pinyi Yang. <em>Heritage Science, 2024</em><br>
    <br><strong>TLDR:</strong> Paper-based artifacts hold significant cultural and social values. 
    However, paper is intrinsically fragile to microorganisms, such as mold, due to its cellulose composition. Mold not only can damage papers‚Äô structural integrity and pose significant challenges to conservation works
    but also may subject individuals attending the contaminated artifacts to health risks. Current conservation practices with mold-contaminated artifacts have
    little to no pre-screening, and the cleaning techniques are
    usually broad-spectrum rather than strain-specific. This study investigated the feasibility of using a convolutional neural network (CNN) for fast in-situ recognition and classification of mold on paper.<br>
  </td>
</tr>  

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/AI in AM.png'>
      <div class="bubble-container"></div>
        <span class="bubble highlight">Smart Manufacturing</span>
        <span class="bubble highlight">Multimodal AI</span>
        <span class="bubble">Knowledge Graph</span>
        <span class="bubble">Decision Support System</span>
      </div>
    </div>
  </td>
  <td class="colright">
    <a href="">
      <span class="papertitle">New Era Towards Autonomous Additive Manufacturing: A Review of Recent Trends and Future Perspectives</span>
    </a><br>
    Haolin Fan, <strong>Chenshu Liu</strong>, Shijie Bian, Changyu Ma, Xuan Liu, Marshall Doyle, Thomas Lu, Lianyi Chen, Jerry Ying Hsi Fuh, Wen Feng Lu, Bingbing Li. <em>International Journal of Extreme Manufacturing (accepted for publication), 2024</em><br>
    <br><strong>TLDR:</strong> The Additive Manufacturing (AM) landscape has significantly transformed in alignment with Industry 4.0 principles, primarily driven by the integration of Artificial Intelligence (AI) and Digital Twin (DT). 
    This review paper examines current solutions in Intelligent Additive Manufacturing, emphasizing control, monitoring and process autonomy, and end-to-end process integration. 
    This paper addresses the lifelong learning and self-optimization capabilities of AI agents. As manufacturing evolves, this paper posits that the future of AM will be characterized by a symbiotic relationship between advanced autonomy and human expertise, fostering a more adaptive and autonomous future manufacturing ecosystem.<br>
  </td>
</tr>  

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/practical_experiment_1.jpg'>
      <div class="bubble-container"></div>
        <span class="bubble highlight">Smart Manufacturing</span>
        <span class="bubble highlight">Vision Language Model</span>
        <span class="bubble">Decision Support System</span>
      </div>
    </div>
  </td>
  <td class="colright">
    <a href="">
      <span class="papertitle">MaViLa: Unlocking New Potentials in Smart Manufacturing through Vision Language Models</span>
    </a><br>
    Haolin Fan, <strong>Chenshu Liu</strong>, Neville Elieh Janvisloo ,Shijie Bian ,Jerry Ying Hsi Fuh ,Wen Feng Lu ,Bingbing Li. <em>Journal of Manufacturing Systems  (under review), 2024</em><br>
    <br><strong>TLDR:</strong> This paper presents MaViLa, a novel Vision Language Model (VLM) designed to unlock new potentials in smart manufacturing. 
    Through rigorous methodology and extensive experiments, MaViLa demonstrates superior performance across various benchmarks compared to other general-purpose VLMs. 
    This enhanced grasping of domain knowledge is attributed to the innovative use of an external vector store during the dataset construction process. 
    Practical experiments, including lab tests and the application of the CAXTON dataset, reveal that MaViLa excels in manufacturing tasks.<br>
  </td>
</tr>

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/Bird Song Classification.png'>
      <div class="bubble-container"></div>
        <span class="bubble highlight">Classification Model</span>
        <span class="bubble highlight">Audio Processing</span>
        <span class="bubble">Machine Learning</span>
      </div>
    </div>
  </td>
  <td class="colright">
    <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0297988#sec002">
      <span class="papertitle">Recognition of bird species with birdsong records using machine learning methods</span>
    </a><br>
    Yi Tang, <strong>Chenshu Liu</strong>, Xiang Yuan. <em>Plos one, 2024</em><br>
    <br><strong>TLDR:</strong> The recognition of bird species through the analysis of their vocalizations is a crucial aspect of wildlife conservation and biodiversity monitoring. In this study, the acoustic features of Certhia americana, Certhia brachydactyla, and Certhia familiaris were calculated to train three machine learning models, Random Forest (RF), Support Vector Machine (SVM), and Extreme Gradient Boosting (XGBoost). The XGBoost model had the best performance among the three models, with the highest accuracy and the highest AUC. The study provides a new approach to bird species recognition that utilizes sound data and acoustic characteristics.<br>
  </td>
</tr>  

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/Education.png'>
      <div class="bubble-container"></div>
        <span class="bubble highlight">Natural Langauge Processing</span>
        <span class="bubble highlight">Semantics Classification</span>
        <span class="bubble">Online Pedagogy</span>
        <span class="bubble">COVID-19 Pandemic</span>
      </div>
    </div>
  </td>
  <td class="colright">
    <a href="https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1326284/full">
      <span class="papertitle">A practical evaluation of online self-assisted previewing architecture on rain classroom for biochemistry lab courses</span>
    </a><br>
    <strong>Chenshu Liu*</strong>, Songbin Ben*, Pinyi Yang, Jiayi Gong, Yin He. <em>Frontiers in Education, 2024</em><br>
    <br><strong>TLDR:</strong> This study investigated the design of the optimal structure of online self-assisting coursework for laboratory courses that can assist students to better prepare for hands-on experiments. 
    Survey was conducted among undergraduate students who took Biochemistry during and post-pandemic. Textmining and semantics classification were performed on students' responses to analyze their emotions towards established online pedagogy frameworks and gain insights in their suggestion for a more effective online learning platform design. 
    We offer a few strategic suggestions that may guide the design of future online resources for laboratory classes such as involving multi-modality media to improve engagement and perfecting the interactive feature to increase its usage by students.<br>
  </td>
</tr>

  </tbody>
</table>

<h2>2023</h2>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <colgroup>
    <col class="colone">
    <col class="coltwo">
  </colgroup>
  <tbody>

<tr> 
  <td class="colleft">
    <div class="one">
      <img src='images/Microorganism Cleaning.png'>
      <div class="bubble-container"></div>
        <span class="bubble highlight">Cultural Heritage Conservation</span>
        <span class="bubble highlight">Mold Removal</span>
        <span class="bubble">Surface Cleaning Agent</span>
        <span class="bubble">Surfactant</span>
        <span class="bubble">Bioenzyme</span>
      </div>
    </div>
  </td>
  <td class="colright">
    <a href="https://link.springer.com/article/10.1186/s40494-023-01083-3">
      <span class="papertitle">A biological cleaning agent for removing mold stains from paper artifacts</span>
    </a><br>
    Qingxia Meng, Xianchao Li, Junqiang Geng, <strong>Chenshu Liu*</strong>, Songbin Ben*. <em>Heritage Science, 2023</em><br>
     <br><strong>TLDR:</strong> Efficient removal of mold stains becomes an important research topic for paper conservation. 
     In this study, a cleaning scheme based on the combination of bioenzymes and biosurfactants was explored. 
     A cleaning agent composed of Sophorolipid and Betaine offer superior deacidification, anti-acidification, anti-aging, and reinforcement capabilities, which can provide extra support to the fibrous structure in addition to cleaning the paper materials. 
     The microbial contamination cleaning agent proposed in this study shows promising application prospects in conserving mold-contaminated paper artifacts.<br>
  </td>
</tr> 

<tr> 
  <td class="colleft">
    <div class="one">
      <img src='images/ABM.JPG'>
      <div class="bubble-container"></div>
        <span class="bubble highlight">Cultural Heritage Conservation</span>
        <span class="bubble highlight">Mold Identification</span>
        <span class="bubble">Computer Vision</span>
        <span class="bubble">Convolutional Neural Network</span>
      </div>
    </div>
  </td>
  <td class="colright">
    <a href="https://www.artbiomatters.org/allmembers/chenshu-liu">
      <span class="papertitle">Ai-Assisted Classification of Microorganism Strains on Paper-Based Cultural Relics (Conference Presentation)</span>
    </a><br>
    <strong>Chenshu Liu</strong>, Chongwen Liu, Allison Wall. <em>Art Bio Matters (ABM) Conference, 2023</em><br>
     <br><strong>TLDR:</strong> Our project that focused on using miscroscopic images of mold stains on paper-based cultural relics using computer vision algorithm was selected to present at the <a href="https://www.artbiomatters.org/">Art Bio Matters</a> conference. 
     The project gained recognition in the cultural heritage conservation community and hold promises in assisting diagnostic procedure in paper-based cultural relic biodeterioration conservation.<br>
  </td>
</tr> 

</tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
         
    <div id="recruiting">  
      <h2 style="padding-top:40px;">Teaching</h2>
    </div>

    <p>
    I am devoted to education of all education levels. I have been teaching for <strong>over 9 years</strong> since the start of senior high. I have taught a broad range of courses, covering subjects: biology (AP, college level), chemistry (AP), calculus (AP, college level), statistics (AP, college level), programming in R, programming in Python, etc. I was the TA for <a href="https://ls7l.lscore.ucla.edu/LAB/">LS23L: Laboratory and Scientific Methods </a> at UCLA in the 2022-2023 academic year.<br>
    </p>
    <p>I am also devoted to enhance public understanding of machine learning and artificial intelligence. I have a personal education channel over <a href="https://www.xiaohongshu.com/user/profile/625a1c60000000000c036ad4">RED</a> to share bite-size knowledge of ML algorithms and AI-related techniques. I also curate a suite of ML-education repositories on Github:</p>
    <ul>
      <li><a href="https://github.com/ChenshuLiu/Machine-Learning-in-Python.git">Machine learning from scratch in Python</a></li>
      <li><a href="https://github.com/ChenshuLiu/Machine-Learning-in-R.git">Machine learning application in R</a></li>
      <li><a href="https://github.com/ChenshuLiu/Deep-Learning-with-R.git">Deep learning application in R</a></li>
    </ul>
  </td>
</tr>

</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:20px">
      <br>
      <p style="font-size:small;">
        This website was built thanks to the help of <a href="https://github.com/czhuang/czhuang.github.io">this</a> source code.
      </p>
    </td>
  </tr>
</tbody></table>
</td>
</tr>
</table>
  </body>
</html>
